{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "QuarantineSports.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gabor132/QuarantineSportsClassifier/blob/master/src/QuarantineSportsModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vayM6pl1hDcd",
        "colab_type": "text",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Quarantine Sports Classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR4pYUzrTAoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# KERAS2ONNX installation\n",
        "#\n",
        "# !pip install -U git+https://github.com/microsoft/onnxconverter-common\n",
        "# !pip install -U git+https://github.com/onnx/keras-onnx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b094aVxOhJnf",
        "colab_type": "text",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ar4UBA-mhFyJ",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import os\n",
        "os.environ['TF_KERAS'] = '1'\n",
        "import keras2onnx\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    IN_COLAB = True\n",
        "    print(\"Running on Google Colab\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running on local machine\")\n",
        "\n",
        "print(\"Using tensorflow version {}\".format(tf.__version__))\n",
        "print(\"Using keras2onnx version {}. Make sure it is 1.7.0\".format(keras2onnx.__version__))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH9guuXpw09P",
        "colab_type": "text"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u1vWejYa1Ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01\n",
        "testing_data_percentage = 0.2\n",
        "nr_of_frames_per_sequence = 15\n",
        "nr_of_keypoints = 25\n",
        "nr_of_values_per_keypoint = 3\n",
        "\n",
        "# Dataset paths\n",
        "DRAGOS_COLAB_DATASET_PATH = \"/content/drive/My Drive/QuarantineSportsDatasets/Dataset/OpenPoseDataset/\"\n",
        "LOCAL_DATASET_PATH = \"../datasets/\"\n",
        "\n",
        "# Model paths\n",
        "DRAGOS_COLAB_MODEL_PATH = \"/content/drive/My Drive/QuarantineSportsDatasets/Model/\"\n",
        "LOCAL_MODEL_PATH = \"../\"\n",
        "\n",
        "current_dataset_path = DRAGOS_COLAB_DATASET_PATH if IN_COLAB else LOCAL_DATASET_PATH\n",
        "current_model_path = DRAGOS_COLAB_MODEL_PATH if IN_COLAB else LOCAL_MODEL_PATH\n",
        "\n",
        "# Get all file names\n",
        "all_files_names = [f for f in listdir(current_dataset_path) if isfile(join(current_dataset_path, f)) and f.endswith('.json')]\n",
        "print(\"Following dataset files have been found: {}\".format(all_files_names))\n",
        "\n",
        "dataset_paths = []\n",
        "for path in all_files_names:\n",
        "  dataset_path = current_dataset_path + path\n",
        "  dataset_paths.append(dataset_path)\n",
        "\n",
        "print(\"All files found: {}\".format(dataset_paths))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4Naobi1hABh",
        "colab_type": "text",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Load Datasets and create X and Y\n",
        "\n",
        "## Important to know\n",
        "\n",
        "1.   Out of open pose we extract for all frames all the 25 keypoints (each keypoint contains the X, Y and score)\n",
        "2.   The Category exists per frame and is 0/1 as in Wrong/Correct as in Not a Push-Up Frame/Is a Push Up Frame\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agJssUaRasLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "y_total = None\n",
        "x_total = None\n",
        "for (index, path) in enumerate(dataset_paths):\n",
        "    print(\"For file at {}\".format(path))\n",
        "    df = pd.read_json(path)\n",
        "    keypoints = df['Keypoints'].values\n",
        "    file_y = df['Category'].values\n",
        "    file_x = []\n",
        "    for k in keypoints:\n",
        "        if k is not None:\n",
        "            newK = np.reshape(np.asarray(k), (25, 3))\n",
        "            file_x.append(newK)\n",
        "        else:\n",
        "            file_x.append(np.reshape(np.zeros(75), (25, 3)))\n",
        "    file_x = np.array(file_x)\n",
        "    print(\"For file at {} found {} frames\".format(path, file_y.shape[0]))\n",
        "    if np.all(x_total is None):\n",
        "        x_total = file_x\n",
        "    else:\n",
        "        x_total = np.vstack((x_total, file_x))\n",
        "    if np.all(y_total is None):\n",
        "        y_total = file_y\n",
        "    else:\n",
        "        y_total = np.hstack((y_total, file_y))\n",
        "print(\"Total Frames: {}\".format(y_total.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_W4X3HoYgOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categories = np.unique(y_total)\n",
        "print(\"Existing categories {}\".format(categories))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcT9VZ-QoK9Q",
        "colab_type": "text"
      },
      "source": [
        "## Splitting the dataset into categories (wrong and correct)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WryRhwOooJvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_by_category = {}\n",
        "for category in categories:\n",
        "  # Get Indexes\n",
        "  y_category_indexes = np.where(y_total == category)\n",
        "  # Get Values\n",
        "  x_category = x_total[y_category_indexes]\n",
        "  data_by_category.update({category: x_category})\n",
        "  print(\"Category {} has {} elements\".format(category, len(x_category)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oq8rBRLoKZc",
        "colab_type": "text"
      },
      "source": [
        "## Remove any frames that only contain empty keypoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbPD8W4EqTxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Check if there are any frames with no keypoints\n",
        "empty_frame = np.zeros((25, 3))\n",
        "for category in data_by_category.keys():\n",
        "    x = data_by_category[category]\n",
        "    empty_frame_indexes = []\n",
        "    for index, x_value in enumerate(x):\n",
        "        if np.array_equal(x_value, empty_frame):\n",
        "            empty_frame_indexes.append(index)\n",
        "    print(\"Category {} has {} empty frames to delete\".format(category, len(empty_frame_indexes)))\n",
        "    x = np.delete(x, empty_frame_indexes, axis=0)\n",
        "    data_by_category.update({category: x})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gveOynHwpAr",
        "colab_type": "text"
      },
      "source": [
        "## Finally, split them into training and testing datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoOaqvu1v8Jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_dataset_categories(dataset_dict):\n",
        "    x_train_total = None\n",
        "    x_test_total = None\n",
        "    x_validate_total = None\n",
        "    y_train_total = None\n",
        "    y_test_total = None\n",
        "    y_validate_total = None\n",
        "    for aux_category in dataset_dict.keys():\n",
        "        x = dataset_dict[aux_category]\n",
        "        y = to_categorical(np.ones((x.shape[0], 1)) * aux_category, 4)\n",
        "        aux_x_train, aux_x_test, aux_y_train, aux_y_test = train_test_split(x, y, test_size=0.2)\n",
        "        aux_x_train, aux_x_validate, aux_y_train, aux_y_validate = train_test_split(aux_x_train, aux_y_train, test_size=0.2)\n",
        "        if x_train_total is None:\n",
        "            x_train_total = aux_x_train\n",
        "            x_test_total = aux_x_test\n",
        "            x_validate_total = aux_x_validate\n",
        "            y_train_total = aux_y_train\n",
        "            y_test_total = aux_y_test\n",
        "            y_validate_total = aux_y_validate\n",
        "        else:\n",
        "            x_train_total = np.concatenate((x_train_total, aux_x_train), axis=0)\n",
        "            x_test_total = np.concatenate((x_test_total, aux_x_test), axis=0)\n",
        "            x_validate_total = np.concatenate((x_validate_total, aux_x_validate), axis=0)\n",
        "            y_train_total = np.concatenate((y_train_total, aux_y_train), axis=0)\n",
        "            y_test_total = np.concatenate((y_test_total, aux_y_test), axis=0)\n",
        "            y_validate_total = np.concatenate((y_validate_total, aux_y_validate), axis=0)\n",
        "    return x_train_total, x_test_total, x_validate_total, y_train_total, y_test_total, y_validate_total\n",
        "\n",
        "\n",
        "\n",
        "x_train, x_test, x_validate, y_train, y_test, y_validate = split_dataset_categories(data_by_category)\n",
        "print(\"Number of training frame sets {} and number of testing frame sets {} and validation frame sets {}\".format(len(x_train), len(x_test), len(x_validate)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKpqu9xqyUuZ",
        "colab_type": "text"
      },
      "source": [
        "## Defining the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAxT0lF6xRoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.Input(shape=(25, 3)))\n",
        "model.add(tf.keras.layers.LSTM(units=75))\n",
        "model.add(tf.keras.layers.ReLU())\n",
        "model.add(tf.keras.layers.Dense(categories.shape[0]))\n",
        "model.add(tf.keras.layers.Softmax(1))\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate),loss='categorical_crossentropy',\n",
        "              metrics=[tf.keras.metrics.categorical_crossentropy])\n",
        "model.summary()\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0scvn6pXTNg",
        "colab_type": "text"
      },
      "source": [
        "# Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gns6OKcO6zsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training\n",
        "print(\"\\n# Training\")\n",
        "model.fit(x_train, y_train, epochs=100, batch_size=50, verbose=1, validation_data=(x_validate, y_validate))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvvSYkF2YA0P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "4119e39b-8767-4b10-b0ae-f87cfca05b8f"
      },
      "source": [
        "print('\\n# Evaluate')\n",
        "result = model.evaluate(x_test, y_test, batch_size=1, verbose=1)\n",
        "print(result)\n",
        "dict(zip(model.metrics_names, result))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate\n",
            "1200/1200 [==============================] - 3s 2ms/step - loss: 0.4112 - categorical_crossentropy: 0.4112\n",
            "[0.4112292230129242, 0.4112292230129242]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'categorical_crossentropy': 0.4112292230129242, 'loss': 0.4112292230129242}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68l-WvCuYA7u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "74116b84-8adf-4560-a49d-c8ebcf785664"
      },
      "source": [
        "predict = model.predict(x_test)\n",
        "nr_correct = {0:0, 1:0, 2:0, 3:0}\n",
        "nr_wrong = {0:0, 1:0, 2:0, 3:0}\n",
        "for (index, p) in enumerate(predict):\n",
        "  p_category = np.where(p == np.max(p))[0][0]\n",
        "  e_category = np.where(y_test[index] == np.max(y_test[index]))[0][0]\n",
        "  if p_category == e_category:\n",
        "    nr_correct.update({e_category: nr_correct[e_category] + 1})\n",
        "  else:\n",
        "    nr_wrong.update({e_category: nr_wrong[e_category] + 1})\n",
        "print(\"Total tested {}\".format(len(predict)))\n",
        "for c in nr_correct.keys():\n",
        "  print(\"{} Correct/Wrong {}/{} - {}%\".format(c, nr_correct[c],nr_wrong[c] + nr_correct[c], int(100* nr_correct[c] / (nr_correct[c] + nr_wrong[c]))))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total tested 1200\n",
            "0 Correct/Wrong 667/684 - 97%\n",
            "1 Correct/Wrong 259/302 - 85%\n",
            "2 Correct/Wrong 118/159 - 74%\n",
            "3 Correct/Wrong 31/55 - 56%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3uAeb4Em6NN",
        "colab_type": "text"
      },
      "source": [
        "## Saving Model to JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_xj1aCJX_Vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(current_model_path + \"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(current_model_path + \"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71uefXeZRHC8",
        "colab_type": "text"
      },
      "source": [
        "# Convert Model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHg7iQgJRGdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "onnx_model = keras2onnx.convert_keras(model)\n",
        "keras2onnx.save_model(onnx_model, current_model_path + \"model.onnx\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SebX2gYurL_B",
        "colab_type": "text"
      },
      "source": [
        "## Finish"
      ]
    }
  ]
}